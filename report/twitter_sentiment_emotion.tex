%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Twitter Sentiment and Emotion Analysis}

\author{Oihane Cantero \\
  UPV/EHU \\
  \texttt{email@domain} \\\And
  Julen Etxaniz \\
  UPV/EHU \\
  \texttt{email@domain} \\\And
  Jose Javier Saiz \\
  UPV/EHU \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
This corpus is a monolingual resource of unique tweets about product reviews, with each tweet paired with an adversarial sentence. Both original and adversarial tweets are manually and automatically annotated according to sentiment polarity (positive, negative, neutral). The objective is to compare and evaluate both types of annotation.
\end{abstract}

\section{Introduction}

This corpus is a monolingual resource of unique English tweets about technology, with each tweet paired with an adversarial sentence. Both original and adversarial tweets are manually and automatically annotated according to sentiment polarity (positive, negative, neutral) and emotions (joy, anger, sadness, optimism). For automatic annotation, the TweetEval \cite{barbieri-etal-2020-tweeteval} evaluation framework  was used, which covers both sentiment polarity and stance detection for Twitter-specific data, among other classification tasks. The objective is to compare and evaluate both types of annotation.

The aim of this resource is to compare the annotations of tweets about technology and their adversarial sentences. It will also be interesting to compare these annotations when we do it by hand or automatically. The results will show whether the models are affected in their performance on classification tasks when faced with adversarial samples. If performance is significantly compromised by this phenomenon, improvements in the criteria used to generate the models may be proposed.

\section{Related work}

\section{Material and methods}

\subsection{Data statement}

\cite{bender-friedman-2018-data}

The resource and code are publicly available at GitHub\footnote{\url{https://github.com/juletx/twitter-sentiment}}. The code is licensed under the MIT open source license. All non-code materials provided are made available under the terms of the CC BY 4.0 license
(Creative Commons Attribution 4.0 International license).

\subsubsection*{A. CURATION RATIONALE}
The corpus is a monolingual test set intended to evaluate text classification models for their ability to estimate sentiment and emotion. This is based on their performance against regular documents and adversarial phenomena. It contains a sample of 140 tweets about technology obtained from the Twitter API via the Tweepy package. We exclude retweets, quotes and replies to get better tweets. We also exclude tweets that have links to reduce spam tweets. The exact search query used is:  \textit{context:65.848920371311001600 lang:en -is:retweet -is:quote -is:reply -has:links}. The goal for using these parameters to collect the text samples was to obtain as many opinions with pronounced sentiments as possible, so that the manual annotation would be as accurate and homogeneous as possible.

\subsubsection*{B. LANGUAGE VARIETY}
Data extraction was made using the IETF language tag for English \textit{en} (ISO 639-1). This language label does not refer to any geographic region or specific variety of English, but since the demographic group of speakers is mostly from the United States, as explained in the section 3.1-C, the language code is \textit{en-US} (ISO 639-1/ ISO 3166-1 alpha-2), which refers to all American English varieties.

\subsubsection*{C. SPEAKER DEMOGRAPHIC}
Tweets are included upon language and semantic content. Any other type of information or demographic data cannot be controlled for selection, which means that this corpus is not based on specific speakers, but on randomly chosen instances from a larger collection.
Therefore, we must rely on general Twitter demographics, which include a majority of speakers which are male (70.4\%) and ranges between 18 and 49 years old (78.7\%). Tweet metadata sometimes provides location information, which for our sample suggests that a significant number of the instances originate in the United States. This information is relevant for determining the prevailing language variety in the corpus.

\subsubsection*{D. ANNOTATOR DEMOGRAPHIC}
The annotation was made by three Spanish MSc students with training in linguistics and computer science.

\subsubsection*{E. SPEECH SITUATION}
Tweets are characterized by being short, spontaneous and conversational in nature. That is, opinions are concise and sentiments are condensed into a few words, which is important for the corpus because it guarantees that the annotation will be carried out and analyzed accurately. Tweets are also time-sensitive texts, which means that the content the instances is only relevant to the time in which they were written.

\subsubsection*{F. TEXT CHARACTERISTICS}
The sample includes tweets made before February 1. The Twitter context “Interests and Hobbies Vertical: Technology'' (domain and entity ID: 65.848920371311001600) was used as the thematic parameter. This context includes “top-level interests and hobbies groupings” about “technology and computing”, as indicated in the description of the metadata context.

\subsection{Guidelines}

We defined very simple initial guidelines before starting the annotation process. The tweets were annotated according to the sentiment (positive, negative, neutral) and emotion (joy, anger, sadness, optimism) that was inferred by the semantic content. Only one label per tweet is allowed for both tasks. We read tweets one by one and annotate both tags.

After the first annotation, we realised that we needed to update the guidelines to increase the annotator agreement. On the one hand, there were some misunderstandings with words and symbols. For example, \&lt;3 is used as a heart and \&gt; equals greater than. On the other hand, there were some were some cases where specific criteria had to be decided to select a label.

When annotating sentiments, some tweets seemed to be ironic or sarcastic, and it was difficult to decide a tag. Since irony makes the "intended meaning [...] appear on the surface to express the opposite" \cite{OUP2022}, we decided for those cases expressing irony or sarcasm to label the opposite sentiment to the overall sentiment when taken literally. 

Another problem for which a clear guideline was needed was tweets with contradicting sentiments. For this case, we decided to keep the overall sentiment to avoid using too many neutral tags.

When there is no clear emotion, we decided to select the \textit{joy} tag because it is the term with the broadest meaning among the possible options. Also, if there is uncertainty between the \textit{joy} and \textit{optimism} sentiments, we also decided to select \textit{joy} for the same reason.

These guidelines are summarized and exemplified in Table \ref{table:1}.

\begin{table*}[ht]
\centering
\begin{tabular}{|l|l|lc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Problem}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Solution}}} & \multicolumn{2}{c|}{\textbf{Example}} \\ \cline{3-4} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{Sentence}} & \textbf{Label} \\ \hline
Irony/ Sarcasm & \begin{tabular}[c]{@{}l@{}}Use opposite sentiment \\ to the literal one\end{tabular} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}"oh yeah tesla well what about a \\ car that just logs into your tiktok \\ acct and drives you to starbucks"\end{tabular}} & negative \\ \hline
\begin{tabular}[c]{@{}l@{}}Contradicting \\ sentiments\end{tabular} & \begin{tabular}[c]{@{}l@{}}Prefer overall sentiment \\ over the "neutral" tag\end{tabular} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}"Finally managed to move my \\ business email from google \\ hosting to another host. So \\ stressful and difficult. The whole \\ internet is so hard! At least the \\ metaverse is coming, I'm confident \\ that will make everything easy and \\ good, phew."\end{tabular}} & positive \\ \hline
Unclear emotion & \begin{tabular}[c]{@{}l@{}}Prefer "joy" tag for being \\ broader in sense\end{tabular} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}"All the software I create will be \\ free and open source, but that \\ doesn't necessarily mean I won't \\ write cryptic software for some \\ of my projects"\end{tabular}} & joy \\ \hline
\end{tabular}
\caption{Defined guidelines after the Inter Annotator Agreement with examples.}
\label{table:1}
\end{table*}

\subsection{Inter Annotator Agreement}

Initially, a sample of the first 20 instances was shared among the annotators. These initial tweets were used to calculate Inter Annotator Agreement, after which we made modifications to the guidelines and scope of the work. 

The ITA between the three annotators and the automatic results was calculated for the 20 shared tweets. We also calculated the agreement between each annotator and the automatic results for the 40 tweets that were annotated individually.

The Kappa scores were computed to see if we agreed in the annotations of both sentiment and emotions, and if we agreed with the automatic annotations.

Thereafter each annotator was given a different set of 40 tweets for annotation. The same methodology was followed, where the manual annotations were used to evaluate the accuracy of the models.

\subsection{Automatic annotation}

These tweets were also labelled using automatic models. Models trained for the TweetEval \cite{barbieri-etal-2020-tweeteval} evaluation framework were used. This framework covers both sentiment polarity and stance detection for Twitter-specific data, among other classification tasks. Two HuggingFace models that are fine-tuned for each task were used for evaluation.

Finally, we will compare the results and extract some conclusions about the accuracy of the models and the guidelines used to train them. We will visualize the percentage of tweets from each class in the manual and automatic annotations. We will also visualize the most common words of each class in word clouds.

\subsection{Adversarial examples}

We will create some adversarial tweets automatically to measure the impact in the metrics of the models. The aim of these adversarial examples is to confuse the models while maintaining the manual label. Therefore, if we generate the adversarial examples correctly, they should have the same meaning and sense, so there is no need to annotate them. As these attacks can easily be replicated, we decided not to include them in the final resource.

We used the Adversary\footnote{\url{https://github.com/airbnb/artificial-adversary}} library to generate 8 generic attacks. These attacks are not specifically prepared for our models, they are simple attacks that can be tested on any type of model. Words are selected with a probability of 0.3 and the selected attack is applied to those words. Attacks can also be combined, but we decide to apply them separately for simplicity.

\begin{enumerate}
    \item Swapping words (swap\_words)
    \item Removing spacing between words (remove\_spacing)
    \item Replacing letters with similar-looking symbols (letter\_to\_symbol)
    \item Swapping letters (swap\_letters)
    \item Inserting punctuation (insert\_punctuation)
    \item Inserting duplicate characters (insert\_duplicate\_characters)
    \item Deleting characters (delete\_characters)
    \item Changing case (change\_case)
\end{enumerate}

An example of each attack can be seen in Table...

\section{Results}

\subsection{Inter Annotator Agreement}

\subsection{Automatic annotation metrics}

\section{Conclusions}

\bibliography{twitter_sentiment_emotion}
\bibliographystyle{acl_natbib}

\end{document}
